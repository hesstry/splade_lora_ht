# @package config

# lr: 1e-4
seed: 123
# gradient_accumulation_steps: 4
# weight_decay: 0.01
validation_metrics: [ MRR@10, recall@100, recall@200, recall@500 , recall@1000]
pretrained_no_yamlconfig: false
# nb_iterations: 300000
# train_batch_size: 4  # number of gpus needs to divide this
eval_batch_size: 16
index_retrieve_batch_size: 16
# record_frequency: 10000
# train_monitoring_freq: 500
warmup_steps: 6000
# max_length: 512
# fp16: true
matching_type: splade
monitoring_ckpt: MRR@10  # or e.g. MRR@10

# hf:
#   training:
#     resume_from_checkpoint: false
#     ddp_find_unused_parameters: false
#     fp16: true
#     logging_steps: 5000
#     save_strategy: steps
#     save_steps: 5000
#     dataloader_drop_last: True
#     # num_train_epochs: 1
#     max_steps: 1
#     warmup_ratio: 0.01
#     mse_margin: true
#     weight_decay: 0
#     gradient_accumulation_steps: 8
#   model:
#     dense: true
#     shared_weights: true
#     dense_pooling: cls
#   data:
#     distillation: true
#     n_negatives: 4
#   token: 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